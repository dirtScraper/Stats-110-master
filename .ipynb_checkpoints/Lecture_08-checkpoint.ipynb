{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8: Random Variables and Their Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Basic Types of Random Variables\n",
    "\n",
    "There are two basic types of random variables.\n",
    "\n",
    "* _Discrete random variables_ are random variables whose values can be enumerated, such as $a_1, a_2, \\dots , a_n$. Here, _discrete_ does not necessarily mean only integers. \n",
    "\n",
    "* _Continuous random variables_, on the other hand, can take on values in $\\mathbb{R}$.\n",
    "\n",
    "Note that there are other types that might mix the two definitions.\n",
    "\n",
    "\n",
    "### Cumulative distribution function\n",
    "\n",
    "Given $X \\le x$ is an event, and a function $F(x) = P(X \\le x)$.\n",
    "\n",
    "Then $F$ is known as the __cumulative distribution function__ (CDF) of $X$. In contrast with the probability mass function, the CDF is more general, and is applicable for both discrete and continuous random variables.\n",
    "\n",
    "![title](images/L0801.png)\n",
    "\n",
    "### Probability mass function\n",
    "\n",
    "The __probability mass function__ (PMF) is $P(X=a_j)$, for all $\\text{j}$. A pmf must satisfy the following conditions:\n",
    "\n",
    "\\begin{align}\n",
    "  P_j &\\ge 0 \\\\\n",
    "  \\sum_j P_j &= 1\n",
    "\\end{align}\n",
    "\n",
    "![title](images/L0802.png)\n",
    "\n",
    "In practice, using a PMF is easier than a CDF, but a PMF is only applicable in the case of __discrete random variables__ only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisiting the Binomial Distribution $Bin(n,p)$\n",
    "\n",
    "$X \\sim Bin(n, p)$, where\n",
    "\n",
    "* $n$ is an integer greater than 0\n",
    "* $p$ is between 0.0 and 1.0\n",
    "\n",
    "Here are a few different ways to explain the Binomial distribution.\n",
    "\n",
    "### Explanation with a story\n",
    "\n",
    "$X$ is the number of successes in $n$ _independent_ $Bern(p)$ trials.\n",
    "\n",
    "### Explanation using sum of indicator random variables\n",
    "\n",
    "\\begin{align}\n",
    "  &X = X_1 + X_2 + \\dots + X_j \n",
    "  & &\\text{where } X_j =\n",
    "  \\begin{cases}\n",
    "    1, &\\text{if } j^{\\text{th}} \\text{ trial succeeds} \\\\\n",
    "    0, &\\text{ otherwise }\n",
    "  \\end{cases} \\\\\n",
    "  & & &\\text{and } X_1, X_2, \\dots , X_j \\text{ are i.i.d Bern(n,p)}\n",
    "\\end{align}\n",
    "\n",
    "### Explanation using a probability mass function\n",
    "\n",
    "\\begin{align}\n",
    "  P(X=k) = \\binom{n}{k} p^k q^{n-k} \\text{, where } q = 1 - p \\text{ and } k \\in \\{0,1,2, \\dots ,n\\}\n",
    "\\end{align}\n",
    "\n",
    "A probability mass function sums to 1. Note that\n",
    "\n",
    "\\begin{align}\n",
    "  \\sum_{k=0}^n \\binom{n}{k} p^k q^{n-k} &= (p + q)^n & &\\text{by the Binomial Theorem} \\\\\n",
    "  &= (p + 1 - p)^n \\\\\n",
    "  &= 1^n \\\\\n",
    "  &= 1\n",
    "\\end{align}\n",
    "so the explanation by PMF holds. By the way, it is this relation to the [Binomial Theorem](https://en.wikipedia.org/wiki/Binomial_theorem) that this distribution gets its name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum of Two Binomial Random Variables\n",
    "\n",
    "Recall that we left Lecture 7 with this parting thought:\n",
    "    \n",
    "\\begin{align}\n",
    "    & X \\sim \\text{Bin}(n,p) \\text{, } Y \\sim \\text{Bin}(m,p) \\rightarrow X+Y \\sim \\text{Bin}(n+m, p)\n",
    "\\end{align}\n",
    "\n",
    "Let's see how this is true by using the three explanation approaches we used earlier.\n",
    "\n",
    "### Explanation with a story\n",
    "\n",
    "$X$ and $Y$ are functions, and since they both have the same sample space $S$ and the same domain. \n",
    "\n",
    "$X$ is the number of successes in $n \\text{ Bern}(p)$ trials.\n",
    "\n",
    "$Y$ is the number of successes in $m \\text{ Bern}(p)$ trials.\n",
    "\n",
    "$\\therefore X + Y$ is simply the sum of successes in $n+m \\text{ Bern}(p)$ trials.\n",
    "\n",
    "### Explanation using a sum of indicator random variables\n",
    "\n",
    "\\begin{align}\n",
    "    &X = X_1 + X_2 + \\dots + X_n, ~~~~ Y = Y_1 + Y_2 + \\dots + Y_m \\\\\n",
    "    &\\Rightarrow X+Y = \\sum_{i=1}^n X_j + \\sum_{j=1}^m Y_j \\text{, which is } n + m \\text{ i.i.d. Bern}(p) \\\\\n",
    "    &\\Rightarrow \\text{Bin}(n + m, p)\n",
    "\\end{align}\n",
    "\n",
    "### Explanation using a probability mass function\n",
    "\n",
    "We start with a PMF of the _convolution_ $X+Y=k$\n",
    "\n",
    "\\begin{align}\n",
    "    P(X+Y=k) &= \\sum_{j=0}^k P(X+Y=k|X=j)P(X=j) & &\\text{wishful thinking, assume we know } x \\text{ and apply LOTP} \\\\\n",
    "    &= \\sum_{j=0}^k P(Y=k-j|X=j) \\binom{n}{j} p^j q^{n-j} & &\\text{... but since events }Y=k-j \\text{ and } X=j \\text{ are independent}\\\\\n",
    "    &= \\sum_{j=0}^k P(Y=k-j) \\binom{n}{j} p^j q^{n-j} & &P(Y=k-j|X=j) = P(Y=k-j) \\\\\n",
    "    &= \\sum_{j=0}^k \\binom{m}{k-j} p^{k-j} q^{m-k+j}~~~ \\binom{n}{j} p^j q^{n-j} \\\\\n",
    "    &= p^k q^{m+n-k} \\underbrace{\\sum_{j=0}^k \\binom{m}{k-j}\\binom{n}{j}}_{\\text{Vandermonde's Identity}} & &\\text{see Lecture 2, Story Proofs, Ex.3} \\\\\n",
    "    &= \\binom{m+n}{k} p^k q^{m+n-k} ~~~~ \\blacksquare\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypergeometric Distribution\n",
    "\n",
    "A common mistake with the Binomial distribution is forgetting that\n",
    "\n",
    "* the trials are independent\n",
    "* they have the same probability of success\n",
    "\n",
    "#### Example: How Many Aces in a 5-card Hand?\n",
    "\n",
    "A 5-card hand out of a standard 52-card deck of playing cards, what is the the distribution (PMF or CDF) of the number of aces.\n",
    "\n",
    "Let $X = (\\text{# of aces})$.\n",
    "\n",
    "Find $P(X=k)$ where $x \\in \\text{\\{0,1,2,3,4\\}}$. At this point, we can conclude that the distribution is __not__ Binomial, _as the trials are not independent_.\n",
    "\n",
    "The PMF is given by\n",
    "\\begin{align}\n",
    "   P(X=k) &= \\frac{\\binom{4}{k}\\binom{48}{5-k}}{\\binom{52}{5}}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "#### Example: Sampling White/Black Marbles\n",
    "\n",
    "There are $b$ black and $w$ white marbles. Pick a random sample of $n$ marbles. What is the distribution on the # white marbles?\n",
    "\n",
    "\\begin{align}\n",
    "    P(X=k) &= \\frac{\\binom{w}{k}\\binom{b}{n-k}}{\\binom{w+b}{n}}\n",
    "\\end{align}\n",
    "\n",
    "The distribution in the examples above is called the __Hypergeometric distribution__. In the hypergeometric distribution, we sample _without replacement_, and so the trials cannot be independent.\n",
    "\n",
    "Note that when the total number of items in our sample is exceedingly large, it becomes highly unlikely that we would choose the same item _more than once_. Therefore, it doesn't matter if you are sampling with replacement or without, suggesting that in this case the hypergeometric behaves as the binomial.\n",
    "\n",
    "#### Is the Hypergeometric distribution a valid PMF?\n",
    "\n",
    "_Is the hypergeometric non-negative?_\n",
    "\n",
    "Yes, obviously.\n",
    "\n",
    "_Does the PMF sum to 1?_\n",
    "\n",
    "\\begin{align}\n",
    "    P(X=k) &=  \\sum_{k=0}^w \\frac{\\binom{w}{k}\\binom{b}{n-k}}{\\binom{w+b}{n}} \\\\\n",
    "    &= \\frac{1}{\\binom{w+b}{n}} \\sum_{k=0}^w \\binom{w}{k}\\binom{b}{n-k} & &\\text{since }\\binom{w+b}{n} \\text{ is constant} \\\\\n",
    "    &= \\frac{1}{\\binom{w+b}{n}} \\binom{w+b}{n} & &\\text{by Vandermonde's Identity } \\\\\n",
    "    &= 1 ~~~~ \\blacksquare\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Appendix A: Independent, Identically Distributed\n",
    "\n",
    "A sequence of random variables are independent and identically distributed (i.i.d) if each r.v. has the same probability distribution and all are mutually independent.\n",
    "\n",
    "Think of the Binomial distribution where we consider a string of coin flips. The probability $p$ for head's is the *same across all coin flips*, and seeing a head (or tail) in a previous toss *does not affect any of the coin flips that come afterwards*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
